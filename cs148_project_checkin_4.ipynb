{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are some explanations relating to the numbered questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We chose 'Learning_Disabilities' as our binary categorical response variable. Its values are 'Yes' and 'No'\n",
    "\n",
    "2. We chose 'Exam_Score' as our predictor varriable. We expect to see some sort of negative correlation, which means that, as Exam_Score increases, the likelihood of Learning_Disabilities being 'Yes' should decrease.\n",
    "\n",
    "3. See code\n",
    "4. See code\n",
    "\n",
    "5. We initially ran the code with the default threshold of 0.5. We then tested lower thresholds to see if they would improve the model's ability to identify a Positive case. Upon testing the lower thresholds, the accuracy of the model fell significantly. So, we decided to proceed with the default threshold and our model obtained an accuracy of roughly 89%. This is a great metric, but it is worth noting that the model had a TPR of 0.0 and a TNR of 1.0, meaning that the model simply predicted 'No' for all cases of having a 'Learning Disability.' This speaks a lot towards the skew of the dataset, as the model technically performed well despite making the same, majority based decision for all data points. As the correlation between our predictor and response variable was quite slight (around -0.08), it makes sense that the model chose a blanket answer to predict the majority of cases.\n",
    "This dataset just simply does not have great binary categorical options, and more data points with a learning disability would help to test the model more rigorously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, roc_auc_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "complete_data = pd.read_csv(\"StudentPerformanceFactors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in Learning_Disabilities: 0\n",
      "NaNs in Exam_Score: 0\n",
      "Infs in Learning_Disabilities: 0\n",
      "Infs in Exam_Score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/p3qb95jn0gs423m7z80y04l00000gn/T/ipykernel_2271/1543863760.py:15: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, take code from previous check-in to clean the data to have our get our updated data frame\n",
    "# Although during this regression modeling, we will only use a subset of these rows, they are all included to create our training, validation, and test data sets\\\n",
    "# These are the features we selected at the beginning of the project to use in our modeling\n",
    "\n",
    "# 1) \"Learning_Disabilities\" is a binary feature, so we'll use this as our response variable\n",
    "# 2) We'll be using \"Exam_Score\" as our predictor for this classification model\n",
    "\n",
    "#putting column name in a variable for now cuz idk which were using\n",
    "predictor_variable = \"Exam_Score\"\n",
    "response_variable = 'Learning_Disabilities'\n",
    "main_features = [response_variable, predictor_variable]\n",
    "data = complete_data[main_features]\n",
    "\n",
    "# making the response variable have a binary encoding\n",
    "data[response_variable] = data[response_variable].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Check to see if data cleaning needed\n",
    "nans_response = data[response_variable].isnull().sum()\n",
    "print(f\"NaNs in {response_variable}: {nans_response}\")\n",
    "\n",
    "nans_predictor = data[predictor_variable].isnull().sum()\n",
    "print(f\"NaNs in {predictor_variable}: {nans_predictor}\")\n",
    "\n",
    "infs_response = np.isinf(data[response_variable]).sum()\n",
    "print(f\"Infs in {response_variable}: {infs_response}\")\n",
    "\n",
    "infs_predictor = np.isinf(data[predictor_variable]).sum()\n",
    "print(f\"Infs in {predictor_variable}: {infs_predictor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same code from the previous check-in, we'll split the data into training, validation, and test sets\n",
    "# Next, divide the new dataframe into 3 different data sets using a 60:20:20 split\n",
    "# We chose 60:20:20 as opposed to 80:10:10 or somwhere in between to decrease the liklihood of overfitting, since the metrics used are potentially subseptible to overfitting\n",
    "\n",
    "\n",
    "train_and_validation_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df, validation_df = train_test_split(train_and_validation_df, test_size=0.25, random_state=42)\n",
    "\n",
    "train_df.to_csv('Student_Performance_train.csv', index=False)\n",
    "validation_df.to_csv('Student_Performance_validation.csv', index=False)\n",
    "test_df.to_csv('Student_Performance_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Correlation: -0.08261872057924036\n",
      "Validation Correlation: -0.11811433308237178\n",
      "Test Correlation: -0.053195325028898434\n"
     ]
    }
   ],
   "source": [
    "## calculate correlation between our two variables using the testing set\n",
    "\n",
    "corr_coef, _ = pointbiserialr(train_df[response_variable], train_df[predictor_variable])\n",
    "\n",
    "print(\"Train Correlation:\", corr_coef)\n",
    "\n",
    "## calculate correlation between our two variables using the validation set\n",
    "\n",
    "corr_coef, _ = pointbiserialr(validation_df[response_variable], validation_df[predictor_variable])\n",
    "\n",
    "print(\"Validation Correlation:\", corr_coef)\n",
    "\n",
    "## calculate correlation between our two variables using the test set\n",
    "\n",
    "corr_coef, _ = pointbiserialr(test_df[response_variable], test_df[predictor_variable])\n",
    "\n",
    "print(\"Test Correlation:\", corr_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3525    1]\n",
      " [ 433    4]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m true_positive_rate \u001b[38;5;241m=\u001b[39m recall_score(y_train, y_pred)  \u001b[38;5;66;03m# TPR\u001b[39;00m\n\u001b[1;32m     19\u001b[0m true_negative_rate \u001b[38;5;241m=\u001b[39m recall_score(y_train, y_pred, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# TNR\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_train, y_pred)  \u001b[38;5;66;03m# F1 Score\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = train_df[[predictor_variable]]\n",
    "y_train = train_df[response_variable]\n",
    "\n",
    "# Training the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = rf_model.predict(x_train)\n",
    "\n",
    "# Calculating the confusion matrix and other metrics\n",
    "c_m = confusion_matrix(y_train, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(c_m)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "prediction_error = 1 - accuracy\n",
    "true_positive_rate = recall_score(y_train, y_pred)  # TPR\n",
    "true_negative_rate = recall_score(y_train, y_pred, pos_label=0)  # TNR\n",
    "f1 = f1_score(y_train, y_pred)  # F1 Score\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy}\")\n",
    "print(f\"Prediction Error: {prediction_error}\")\n",
    "print(f\"True Positive Rate (TPR): {true_positive_rate}\")\n",
    "print(f\"True Negative Rate (TNR): {true_negative_rate}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# 5-fold cross-validation on validation set\n",
    "X_val = validation_df[[predictor_variable]]\n",
    "y_val = validation_df[response_variable]\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate AUC, accuracy and F1 across the folds\n",
    "auc_scores = cross_val_score(rf_model, X_val, y_val, cv=cv, scoring='roc_auc')\n",
    "accuracy_scores = cross_val_score(rf_model, X_val, y_val, cv=cv, scoring='accuracy')\n",
    "f1_scores = cross_val_score(rf_model, X_val, y_val, cv=cv, scoring='f1')\n",
    "\n",
    "print(\"\\nAUC Scores for each fold:\", auc_scores)\n",
    "print(\"Average AUC:\", np.mean(auc_scores))\n",
    "print(\"Accuracy Scores for each fold:\", accuracy_scores)\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"F1 Scores for each fold:\", f1_scores)\n",
    "print(\"Average F1:\", np.mean(f1_scores))\n",
    "\n",
    "# Plot AUC curve\n",
    "rf_model.fit(X_val, y_val)  # Fit model to validation data for ROC curve plotting\n",
    "y_val_proba = rf_model.predict_proba(X_val)[:, 1]  # Probability of the positive class\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_proba)\n",
    "auc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC Curve (AUC = {auc:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash'), name='Random'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Random Forest ROC Curve on Validation Set\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
